{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZDsDHiGqKjL",
        "outputId": "6da6c699-546c-4bc7-d891-203f190d7873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from pretty_midi) (1.22.4)\n",
            "Collecting mido>=1.1.16\n",
            "  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from pretty_midi) (1.16.0)\n",
            "Building wheels for collected packages: pretty_midi\n",
            "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592303 sha256=35590232a2353e68b1e5932d90f18278036c7a85f5bc4ef58c27513977a8a9a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/ec/20/b8e937a5bcf1de547ea5ce465db7de7f6761e15e6f0a01e25f\n",
            "Successfully built pretty_midi\n",
            "Installing collected packages: mido, pretty_midi\n",
            "Successfully installed mido-1.2.10 pretty_midi-0.2.10\n"
          ]
        }
      ],
      "source": [
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "path_to_data = '/content/gdrive/My Drive/University/Year 4/CSC413/Project/data.pickle'\n",
        "\n",
        "!pip install pretty_midi\n",
        "!cp \"/content/gdrive/My Drive/University/Year 4/CSC413/Project/data_parser.py\" .\n",
        "\n",
        "from data_parser import create_midi_file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model**"
      ],
      "metadata": {
        "id": "NdkLToGQqYSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicGenRNN(nn.Module):\n",
        "  def __init__(self, hidden_size = 512, num_rnn_layers = 1, bias_rnn = True, pitch_embedding_size=130):\n",
        "    super(MusicGenRNN, self).__init__()\n",
        "    # pitch_vocab_size = number of notes possible + begin sequence token\n",
        "    self.pitch_vocab_size = 128 + 1\n",
        "    # to turn pitch to one hots\n",
        "    self.ident = torch.eye(self.pitch_vocab_size).to(device)\n",
        "\n",
        "    # number of units in RNN encoder\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "    # embedding size for pitch, to turn one hots to continous vectors\n",
        "    self.pitch_embedding_size = pitch_embedding_size\n",
        "    self.pitch_embedding_layer = nn.Linear(self.pitch_vocab_size, self.pitch_embedding_size, bias=False)\n",
        "    \n",
        "    # LSTM params nn.LSTM(input_size, hidden_size, num_layers, bias, batch_first=True, dropout)\n",
        "    # add two to embedding size to account for step and duration\n",
        "    self.rnn = nn.LSTM(self.pitch_embedding_size + 2, hidden_size, num_rnn_layers, bias=bias_rnn, batch_first=True, dropout=0)\n",
        "    \n",
        "    # decoders to get distributions and values for pitch, step and duration\n",
        "    # went with 300 here as it is in between deafult 512 and pitch_vocab_size\n",
        "    self.decoder_pitch1 = nn.Linear(hidden_size, 300)\n",
        "    self.decoder_pitch2 = nn.Linear(300, self.pitch_vocab_size)\n",
        "\n",
        "    # step and duration decoders\n",
        "    self.decoder_step = nn.Linear(hidden_size, 1)\n",
        "    self.decoder_duration = nn.Linear(hidden_size, 1)\n",
        "\n",
        "\n",
        "  def forward(self, input, hidden_in=None):\n",
        "    # parse input\n",
        "    pitch_in = input[:, :, 0].long()\n",
        "    step_in = input[:, :, 1]\n",
        "    dur_in = input[:, :, 2]\n",
        "    # embed pitch\n",
        "    pitch_one_hot = self.ident[pitch_in]\n",
        "    pitch_emb = self.pitch_embedding_layer(pitch_one_hot)\n",
        "    # join everything back together to encode it\n",
        "    inp = torch.concat((pitch_emb, step_in.unsqueeze(2), dur_in.unsqueeze(2)), dim=2)\n",
        "    output, hidden_out = self.rnn(inp, hidden_in) \n",
        "    # decoder everything\n",
        "    out_pitch = self.decoder_pitch2(torch.relu(self.decoder_pitch1(output)))  \n",
        "    out_step = self.decoder_step(output)\n",
        "    out_dur = self.decoder_duration(output)\n",
        "\n",
        "    return torch.concat((out_pitch, out_step, out_dur), dim=2), hidden_out"
      ],
      "metadata": {
        "id": "Gi5bsggAqYEM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading Data**"
      ],
      "metadata": {
        "id": "eZd4xBWWsC1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path_to_data, 'rb') as f:\n",
        "    dataset = pickle.load(f)\n",
        "\n",
        "train_set, validation_set, test_set = dataset\n",
        "\n",
        "print(train_set.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkuRFdiosVPs",
        "outputId": "7c789ffa-107c-49ff-eb88-a3613f2ee51b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3603, 64, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "86530rzKsVyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_distros = []\n",
        "valid_distros = []\n",
        "\n",
        "def train(model, device, train_data, valid_data, batch_size=32, weight_decay=0.0,\n",
        "           learning_rate=0.001, num_epochs=7, checkpoint_path=None):\n",
        "  # get loss function, CE has softmax built in\n",
        "  criterion_pitch = nn.CrossEntropyLoss()\n",
        "  criterion_step = nn.MSELoss()\n",
        "  criterion_dur = nn.MSELoss()\n",
        "  # criterion_step = nn.L1Loss()\n",
        "  # criterion_dur = nn.L1Loss()\n",
        "\n",
        "  # get optimizer\n",
        "  optimizer = optim.Adam(model.parameters(),\n",
        "                          lr=learning_rate,\n",
        "                          weight_decay=weight_decay)\n",
        "  \n",
        "  # get dataloader, load training data, and validation data\n",
        "  train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True)\n",
        "  validation_loader = torch.utils.data.DataLoader(valid_data,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True)\n",
        "  \n",
        "  # learning curve information for plotting\n",
        "  train_iters_list, losses_training, iter_at_epoch, train_acc, val_acc = [], [], [], [], []\n",
        "  valid_iters_list, losses_validation =  [], []\n",
        "  num_iters_train, num_iters_valid = 0, 0\n",
        "\n",
        "  # iterate the given number of epochs\n",
        "  for epoch in range(num_epochs):\n",
        "    # shuffling data done automatically by data loader\n",
        "    for batch_of_sequences in iter(train_loader): # iterate through all data in train loader\n",
        "    # batch_of_sequences is of batch size\n",
        "      # account for smaller last batch\n",
        "      if batch_of_sequences.size()[0] < batch_size:\n",
        "        continue\n",
        "\n",
        "      # compute forward and backward pass\n",
        "      model.train() # ensure model in train mode\n",
        "\n",
        "      # add <BOS>=128, pitch values range 0-127 inclusive\n",
        "      BOS = torch.tensor([128, 0.0, 0.0] * batch_size).reshape(batch_size, 1, 3)\n",
        "      input = torch.concat((BOS, batch_of_sequences), dim=1).to(device)\n",
        "      # only go up to second last input as last one is just predicted never fed in\n",
        "      input = input[:, :-1, :] \n",
        "      out, _ = model(input.float())\n",
        "      out_pitch = out[:, :, :129]\n",
        "      out_step = out[:, :, 129]\n",
        "      out_dur = out[:, :, 130]\n",
        "\n",
        "      targets = batch_of_sequences.float().to(device)\n",
        "      targets_pitch = targets[:, :, 0].long()\n",
        "      targets_step = targets[:, :, 1]\n",
        "      targets_dur = targets[:, :, 2]\n",
        "      \n",
        "      # compute losses\n",
        "      loss_pitch = criterion_pitch(out_pitch.reshape(-1, model.pitch_vocab_size), targets_pitch.reshape(-1).long())\n",
        "      loss_step = criterion_step(out_step, targets_step)\n",
        "      loss_dur = criterion_dur(out_dur, targets_dur)\n",
        "      total_loss = loss_step + loss_dur + loss_pitch\n",
        "      # go backward/grad descent\n",
        "      total_loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # gather plotting data\n",
        "      num_iters_train += 1\n",
        "      losses_training.append(float(loss_pitch.item() + loss_step.item() + loss_dur.item()))\n",
        "      train_iters_list.append(num_iters_train)\n",
        "\n",
        "  \n",
        "    # iterate through all data in valid loader\n",
        "    for batch_of_sequences in iter(validation_loader): \n",
        "      # batch_of_sequences is of batch size\n",
        "        # account for smaller last batch\n",
        "        if batch_of_sequences.size()[0] < batch_size:\n",
        "          continue\n",
        "        # compute forward and backward pass\n",
        "        model.eval() # ensure model in eval mode, no decent here just trying to gain accuarcy\n",
        "\n",
        "        # add <BOS>=128, pitch values range 0-127 inclusive\n",
        "        BOS = torch.tensor([128, 0.0, 0.0] * batch_size).reshape(batch_size, 1, 3)\n",
        "        input = torch.concat((BOS, batch_of_sequences), dim=1).to(device)\n",
        "        # only go up to second last input as last one is just predicted never fed in\n",
        "        input = input[:, :-1, :] \n",
        "        out, _ = model(input.float())\n",
        "        out_pitch = out[:, :, :129]\n",
        "        out_step = out[:, :, 129]\n",
        "        out_dur = out[:, :, 130]\n",
        "\n",
        "        targets = batch_of_sequences.float().to(device)\n",
        "        targets_pitch = targets[:, :, 0].long()\n",
        "        targets_step = targets[:, :, 1]\n",
        "        targets_dur = targets[:, :, 2]\n",
        "        \n",
        "        # compute losses\n",
        "        loss_pitch_v = criterion_pitch(out_pitch.reshape(-1, model.pitch_vocab_size), targets_pitch.reshape(-1).long())\n",
        "        loss_step_v = criterion_step(out_step, targets_step)\n",
        "        loss_dur_v = criterion_dur(out_dur, targets_dur)\n",
        "        total_loss_v = loss_pitch_v + loss_step_v + loss_dur_v\n",
        "        \n",
        "        # gather plotting data\n",
        "        num_iters_valid += 1\n",
        "        losses_validation.append(float(loss_pitch_v.item() + loss_step_v.item() + loss_dur_v.item()))\n",
        "        valid_iters_list.append(num_iters_valid)\n",
        "\n",
        "    # --- epoch ended ---\n",
        "    # check point model\n",
        "    if (checkpoint_path is not None) and num_iters_train > 0:\n",
        "      torch.save(model.state_dict(), checkpoint_path.format(num_iters_train))\n",
        "\n",
        "    # track learning curve info\n",
        "    iter_at_epoch.append(num_iters_train)\n",
        "    cur_train_acc, cur_val_acc = estimate_accuracy(model, train_data, valid_data, 50, compute_set_distros=(len(iter_at_epoch) == 1))\n",
        "    train_acc.append(cur_train_acc * 100)\n",
        "    val_acc.append(cur_val_acc * 100)\n",
        "\n",
        "    print(\"Epoch %d. Iter %d.  [Train Acc %.0f%%, Train Loss %f].  [Valid Acc %.0f%%, Valid Loss %f]\" %\n",
        "              (epoch, iter_at_epoch[-1], train_acc[-1], float((loss_pitch + loss_step + loss_dur).cpu().detach().numpy()),\n",
        "               val_acc[-1], float((loss_pitch_v + loss_step_v + loss_dur_v).cpu().detach().numpy())))\n",
        "  return train_iters_list, losses_training, valid_iters_list, losses_validation, iter_at_epoch, train_acc, val_acc\n",
        "\n",
        "\n",
        "\n",
        "def plot_learning_curve(train_iters_list, losses_training, valid_iters_list, losses_validation, iter_at_epoch, train_acc, val_acc):\n",
        "    \"\"\"\n",
        "    Plot the learning curve.\n",
        "    \"\"\"\n",
        "    plt.title(\"Learning Curve: Train Loss per Iteration\")\n",
        "    plt.plot(train_iters_list, losses_training, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Learning Curve: Valid Loss per Iteration\")\n",
        "    plt.plot(valid_iters_list, losses_validation, label=\"Valid\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Learning Curve: Accuracy per Iteration\")\n",
        "    plt.plot(iter_at_epoch, train_acc, label=\"Train\")\n",
        "    plt.plot(iter_at_epoch, val_acc, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def estimate_accuracy(model, train_set, valid_set, num_samples, compute_set_distros):\n",
        "\n",
        "  # build histogram/distribtution of tokens over train and valid set\n",
        "  global train_distros\n",
        "  global valid_distros\n",
        "\n",
        "  if compute_set_distros:\n",
        "    train_pitch_histo = [0.0001] * 128\n",
        "    valid_pitch_histo = [0.0001] * 128\n",
        "\n",
        "    # discretize step and duration into bins \n",
        "    train_step_histo = [0.0001] * ((185 - 0) // 5)\n",
        "    valid_step_histo = [0.0001] * ((185 - 0) // 5)\n",
        "\n",
        "    train_dur_histo = [0.0001] * int((15 - 0) // 0.5)\n",
        "    valid_dur_histo = [0.0001] * int((15 - 0) // 0.5)\n",
        "\n",
        "    for sequence in train_set:\n",
        "      for token in sequence:\n",
        "        pitch, step, dur = token\n",
        "        train_pitch_histo[int(pitch)] += 1\n",
        "        # use min here to make sure long step or dur well over 3 ends up in upper bin\n",
        "        # use \"step // 0.2 if step > 0 else 0\" to ensure steps/dur < 0 end up in lowest bin\n",
        "        train_step_histo[min(int(step // 5 if step > 0 else 0), len(train_step_histo) - 1)] += 1\n",
        "        train_dur_histo[min(int(dur // 0.5 if step > 0 else 0), len(train_dur_histo) - 1)] += 1\n",
        "\n",
        "    for sequence in valid_set:\n",
        "      for token in sequence:\n",
        "        pitch, step, dur = token\n",
        "        valid_pitch_histo[int(pitch)] += 1\n",
        "        valid_step_histo[min(int(step // 5 if step > 0 else 0), len(valid_step_histo) - 1)] += 1\n",
        "        valid_dur_histo[min(int(dur // 0.5 if step > 0 else 0), len(valid_dur_histo) - 1)] += 1\n",
        "\n",
        "    # set globals so we don't need to compute this more then once\n",
        "    train_distros = [train_pitch_histo, train_step_histo, train_dur_histo] \n",
        "    valid_distros = [valid_pitch_histo, valid_step_histo, valid_dur_histo]\n",
        "    \n",
        "  else:\n",
        "    train_pitch_histo, train_step_histo, train_dur_histo = train_distros\n",
        "    valid_pitch_histo, valid_step_histo, valid_dur_histo = valid_distros\n",
        "\n",
        "  model_pitch_histo = [0.0001] * 128\n",
        "  model_step_histo = [0.0001] * int((185 - 0) // 5)\n",
        "  model_dur_histo = [0.0001] * int((15 - 0) // 0.5)\n",
        "\n",
        "  for _ in range(num_samples):\n",
        "    gen_seq = sample_sequence(model, max_len=100, temperature=0.8)\n",
        "    for token in gen_seq:\n",
        "      pitch, step, dur = token\n",
        "      model_pitch_histo[min(max(int(pitch), 0), len(model_pitch_histo) - 1)] += 1\n",
        "      model_step_histo[min(int(step // 5 if step > 0 else 0), len(model_step_histo) - 1)] += 1\n",
        "      model_dur_histo[min(int(dur // 0.5 if step > 0 else 0), len(model_dur_histo) - 1)] += 1\n",
        "  \n",
        "  # compute distro similarity using JSD as it is symmetric and bounded between 0 and 1\n",
        "  # https://medium.com/geekculture/techniques-to-measure-probability-distribution-similarity-9145678d68a6\n",
        "\n",
        "  train_distro_similarity = (_jensen_shannon_divergence(model_pitch_histo, train_pitch_histo) + \n",
        "                            _jensen_shannon_divergence(model_step_histo, train_step_histo) +\n",
        "                            _jensen_shannon_divergence(model_dur_histo, train_dur_histo)) / 3\n",
        "  valid_distro_similarity = (_jensen_shannon_divergence(model_pitch_histo, valid_pitch_histo) + \n",
        "                            _jensen_shannon_divergence(model_step_histo, valid_step_histo) +\n",
        "                            _jensen_shannon_divergence(model_dur_histo, valid_dur_histo)) / 3\n",
        "  return train_distro_similarity, valid_distro_similarity\n",
        "\n",
        "\n",
        "def _kullback_leibler_divergence(p_probs, q_probs):  \n",
        "    kl_div = p_probs * (np.log(p_probs / q_probs) / np.log(2))\n",
        "    return np.sum(kl_div)\n",
        "\n",
        "def _jensen_shannon_divergence(distro_model, distro_set):\n",
        "    # make numpy arrays\n",
        "    distro_model = np.array(distro_model)\n",
        "    distro_set = np.array(distro_set)\n",
        "    # normalize both distros\n",
        "    distro_model = distro_model / distro_model.sum()\n",
        "    distro_set = distro_set / distro_set.sum()\n",
        "    # average them\n",
        "    avg_distro = (distro_model + distro_set) / 2\n",
        "    # return avg of KL(model_distro, avg) and KL(<valid/train>_distro, avg)\n",
        "    return (_kullback_leibler_divergence(distro_model, avg_distro) \n",
        "                + _kullback_leibler_divergence(distro_set, avg_distro)) / 2\n"
      ],
      "metadata": {
        "id": "gE_iFOLusaeG"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MusicGenRNN(num_rnn_layers=2)\n",
        "model.to(device)\n",
        "# check_point_path = \"/content/gdrive/My Drive/University/Year 4/CSC413/Project/check_pts/ckpt-{}.pk\"\n",
        "plot_data = train(model, device, train_set[:500], validation_set, num_epochs=40, batch_size=20, weight_decay=1e-5)\n",
        "plot_learning_curve(*plot_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "id": "_WPNsjy8wqpY",
        "outputId": "1d4d33d3-0492-42ba-fa96-dddc08975466"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0. Iter 25.  [Train Acc 2%, Train Loss 3.897702].  [Valid Acc 2%, Valid Loss 3.845031]\n",
            "Epoch 1. Iter 50.  [Train Acc 2%, Train Loss 3.800767].  [Valid Acc 2%, Valid Loss 3.763839]\n",
            "Epoch 2. Iter 75.  [Train Acc 3%, Train Loss 3.562606].  [Valid Acc 3%, Valid Loss 3.527675]\n",
            "Epoch 3. Iter 100.  [Train Acc 3%, Train Loss 3.487701].  [Valid Acc 3%, Valid Loss 3.181968]\n",
            "Epoch 4. Iter 125.  [Train Acc 5%, Train Loss 3.194861].  [Valid Acc 4%, Valid Loss 3.341313]\n",
            "Epoch 5. Iter 150.  [Train Acc 5%, Train Loss 2.999497].  [Valid Acc 5%, Valid Loss 3.095354]\n",
            "Epoch 6. Iter 175.  [Train Acc 5%, Train Loss 3.107348].  [Valid Acc 6%, Valid Loss 3.148427]\n",
            "Epoch 7. Iter 200.  [Train Acc 3%, Train Loss 3.201798].  [Valid Acc 3%, Valid Loss 3.017407]\n",
            "Epoch 8. Iter 225.  [Train Acc 4%, Train Loss 2.909214].  [Valid Acc 5%, Valid Loss 3.343892]\n",
            "Epoch 9. Iter 250.  [Train Acc 9%, Train Loss 2.895431].  [Valid Acc 9%, Valid Loss 3.031423]\n",
            "Epoch 10. Iter 275.  [Train Acc 3%, Train Loss 2.878518].  [Valid Acc 4%, Valid Loss 3.028224]\n",
            "Epoch 11. Iter 300.  [Train Acc 3%, Train Loss 2.811725].  [Valid Acc 3%, Valid Loss 3.013323]\n",
            "Epoch 12. Iter 325.  [Train Acc 6%, Train Loss 2.918827].  [Valid Acc 6%, Valid Loss 3.050024]\n",
            "Epoch 13. Iter 350.  [Train Acc 2%, Train Loss 2.733811].  [Valid Acc 3%, Valid Loss 4.874524]\n",
            "Epoch 14. Iter 375.  [Train Acc 2%, Train Loss 2.731666].  [Valid Acc 2%, Valid Loss 3.178665]\n",
            "Epoch 15. Iter 400.  [Train Acc 9%, Train Loss 2.710556].  [Valid Acc 10%, Valid Loss 3.018632]\n",
            "Epoch 16. Iter 425.  [Train Acc 4%, Train Loss 2.734172].  [Valid Acc 3%, Valid Loss 2.812680]\n",
            "Epoch 17. Iter 450.  [Train Acc 6%, Train Loss 2.818805].  [Valid Acc 6%, Valid Loss 3.034360]\n",
            "Epoch 18. Iter 475.  [Train Acc 4%, Train Loss 2.510974].  [Valid Acc 5%, Valid Loss 2.916994]\n",
            "Epoch 19. Iter 500.  [Train Acc 5%, Train Loss 2.533510].  [Valid Acc 5%, Valid Loss 2.863282]\n",
            "Epoch 20. Iter 525.  [Train Acc 6%, Train Loss 2.319766].  [Valid Acc 6%, Valid Loss 2.933720]\n",
            "Epoch 21. Iter 550.  [Train Acc 3%, Train Loss 2.513945].  [Valid Acc 3%, Valid Loss 3.014300]\n",
            "Epoch 22. Iter 575.  [Train Acc 6%, Train Loss 2.155905].  [Valid Acc 7%, Valid Loss 2.745286]\n",
            "Epoch 23. Iter 600.  [Train Acc 4%, Train Loss 2.099453].  [Valid Acc 4%, Valid Loss 2.798101]\n",
            "Epoch 24. Iter 625.  [Train Acc 4%, Train Loss 2.019453].  [Valid Acc 4%, Valid Loss 2.928253]\n",
            "Epoch 25. Iter 650.  [Train Acc 5%, Train Loss 2.004763].  [Valid Acc 5%, Valid Loss 3.117716]\n",
            "Epoch 26. Iter 675.  [Train Acc 4%, Train Loss 1.999038].  [Valid Acc 5%, Valid Loss 3.254004]\n",
            "Epoch 27. Iter 700.  [Train Acc 5%, Train Loss 1.843918].  [Valid Acc 5%, Valid Loss 3.308716]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-8ad994354c58>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# check_point_path = \"/content/gdrive/My Drive/University/Year 4/CSC413/Project/check_pts/ckpt-{}.pk\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplot_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mplot_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-84773094cc52>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_data, valid_data, batch_size, weight_decay, learning_rate, num_epochs, checkpoint_path)\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_step\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_dur\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_pitch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0;31m# go backward/grad descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.load(\"/content/gdrive/My Drive/University/Year 4/CSC413/Project/friday_model_no_eos_more_data.pk\", map_location=torch.device('cpu'))\n",
        "model.load_state_dict(weights)\n",
        "# model.load_state_dict(torch.load(\"/content/gdrive/My Drive/University/Year 4/CSC413/Project/{}.pk\".format(NAME HERE)))"
      ],
      "metadata": {
        "id": "UACSM2Xzwyea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"/content/gdrive/My Drive/University/Year 4/CSC413/Project/{}.pk\".format(\"500-2enc-pre-of\"))"
      ],
      "metadata": {
        "id": "d_x5POF8x1Vn"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sampling/Producing Music**"
      ],
      "metadata": {
        "id": "emW8v0Gvwhe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_sequence(model, input_seq = None, max_len = 100, temperature = 0.8):\n",
        "  model.eval()\n",
        "  inp = torch.Tensor([[128, 0, 0]]).reshape(1,3).float().to(device)\n",
        "  generated_sequence = []\n",
        "  hidden = None\n",
        "  if input_seq is not None:\n",
        "    out, hidden = model(input_seq.unsqueeze(0), hidden)\n",
        "  for p in range(max_len):\n",
        "    output, hidden = model(inp.unsqueeze(0), hidden)\n",
        "\n",
        "    # Sample from the network as a multinomial distribution\n",
        "    output_dist = output[0, 0, :129].data.view(-1).div(temperature).exp()\n",
        "    top_i = int(torch.multinomial(output_dist, 1)[0])\n",
        "    # Add predicted character to string and use as next input \n",
        "    predicted_pitch = [top_i, output[0, 0, 129], output[0, 0, 130]]\n",
        "    if top_i == 129: break\n",
        "    generated_sequence.append(predicted_pitch)\n",
        "    inp = torch.Tensor([[top_i, output[0, 0, 129], output[0, 0, 130]]]).reshape(1, 3).float().to(device)\n",
        "    # print(inp)\n",
        "  if input_seq is not None:\n",
        "    return torch.concat((input_seq.cpu(), torch.tensor(generated_sequence).cpu()), dim=0)\n",
        "  return torch.tensor(generated_sequence)\n"
      ],
      "metadata": {
        "id": "8_G6zW5Zwg21"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = torch.Tensor(validation_set[50]).to(device)\n",
        "gen_seq = sample_sequence(model, max_len=200, temperature=.8, input_seq=None)\n",
        "\n",
        "create_midi_file(\"/content/gdrive/My Drive/University/Year 4/CSC413/Project/currjj4.mid\", gen_seq)\n",
        "create_midi_file(\"/content/gdrive/My Drive/University/Year 4/CSC413/Project/test.mid\", inp)"
      ],
      "metadata": {
        "id": "prXFC_AGxxBl"
      },
      "execution_count": 45,
      "outputs": []
    }
  ]
}