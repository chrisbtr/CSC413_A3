{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5CYRKpOHv2ld"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initial Model**"
      ],
      "metadata": {
        "id": "BtDg9WfT7Llm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicGenRNN(nn.Module):\n",
        "  def __init__(self, hidden_size=512, num_layers=1, bias=True):\n",
        "    super(MusicGenRNN, self).__init__()\n",
        "    # input # pitch, step and duration \n",
        "    # 128 is number of pitch possiblities\n",
        "    self.one_hot_size = 128 + 2 # account for <BOS> and <EOS> this will be the second last and last features of the one hot\n",
        "    self.pitch_embedding_size = 128 + 2\n",
        "    self.hidden_size = hidden_size\n",
        "    # size of embedding plus the 2 cts values step and duration\n",
        "    self.input_size = self.pitch_embedding_size + 2 \n",
        "    # size of discrete one hot plus the 2 cts values step and duration\n",
        "    self.output_size = self.one_hot_size + 2 \n",
        "\n",
        "    # identiy matrix for generating one-hot vectors\n",
        "    self.ident = torch.eye(self.one_hot_size) \n",
        "    self.pitch_embedding = nn.Linear(self.one_hot_size, self.pitch_embedding_size, bias=False)\n",
        "\n",
        "    #self.rnn = nn.LSTM(input_size, hidden_size, num_layers, bias, batch_first=True, dropout)\n",
        "    self.rnn = nn.LSTM(self.input_size, hidden_size, num_layers, bias=bias, batch_first=True, dropout=0)\n",
        "    # a fully-connect layer that outputs a distribution over the next token, given the RNN output\n",
        "    self.decoder = nn.Linear(hidden_size, self.output_size)\n",
        "\n",
        "  def forward(self, input, hidden_in=None):\n",
        "    inp_pitch = input[:, :, 0]\n",
        "    inp_step = input[:, :, 1]\n",
        "    inp_duration = input[:, :, 2]\n",
        "    inp_pitch = inp_pitch.long()\n",
        "    # generate one-hot vector for discrete part of input\n",
        "    one_hot_pitch = self.ident[inp_pitch].float()\n",
        "    # embed the pitch to make it cts\n",
        "    embedded_pitch = self.pitch_embedding(one_hot_pitch)\n",
        "    # make inp = batch_size x sequence_length x 132\n",
        "    inp = torch.concat((embedded_pitch, inp_step.reshape(*inp_step.shape, 1), inp_duration.reshape(*inp_duration.shape, 1)), dim=2)\n",
        "    output, hidden_out = self.rnn(inp, hidden_in) # get the next output and hidden state\n",
        "    output = self.decoder(output) # predict distribution over next tokens\n",
        "    return output, hidden_out"
      ],
      "metadata": {
        "id": "gaX60joVwRaT"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trainning Over Fitting**"
      ],
      "metadata": {
        "id": "XDX1DOWf5g1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "path_to_data = '/content/gdrive/My Drive/University/Year 4/CSC413/Project/data.pickle'\n",
        "\n",
        "with open(path_to_data, 'rb') as f:\n",
        "    dataset = pickle.load(f)\n",
        "\n",
        "train_set, validation_set, test_set = dataset\n",
        "\n",
        "print(train_set.shape)\n",
        "#print(train_set[0])\n",
        "\n",
        "# train contains N samples, of 64 length sequences, each sequence token is length 3.\n",
        "# token in sequence is [pitch, step, duration]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfFaAYG95ggF",
        "outputId": "011685a9-cad9-4685-a0e8-1e97d66df64e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "(3603, 64, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_data, valid_data, batch_size=32, weight_decay=0.0,\n",
        "           learning_rate=0.001, num_epochs=7, checkpoint_path=None):\n",
        "  # get loss function, CE has softmax built in\n",
        "  criterion_pitch = nn.CrossEntropyLoss()\n",
        "  criterion_step = nn.MSELoss()\n",
        "  criterion_duration = nn.MSELoss()\n",
        "  # get optimizer\n",
        "  optimizer = optim.Adam(model.parameters(),\n",
        "                          lr=learning_rate,\n",
        "                          weight_decay=weight_decay)\n",
        "  # get dataloader, load training data\n",
        "  train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True)\n",
        "  # learning curve information for plotting\n",
        "  iters, iter_at_epoch, losses, train_acc, val_acc = [], [], [], [], []\n",
        "  num_iters = 0\n",
        "  # iterate the given number of epochs\n",
        "  for epoch in range(num_epochs):\n",
        "    # shuffling data done automatically by data loader\n",
        "    for batch_of_sequences in iter(train_loader): # iterate through all data in loader\n",
        "    # batch_of_sequences is of batch size\n",
        "      # account for smaller last batch\n",
        "      if batch_of_sequences.size()[0] < batch_size:\n",
        "        continue\n",
        "      # compute forward and backward pass\n",
        "      model.train() # ensute model in train mode\n",
        "\n",
        "      # add <BOS>=128 and <EOS>=129 terms, pitch values range 0-127 inclusive\n",
        "      BOS = torch.tensor([128, 0.0, 0.0] * batch_size).reshape(batch_size, 1, 3)\n",
        "      EOS = torch.tensor([129, 0.0, 0.0] * batch_size).reshape(batch_size, 1, 3)\n",
        "      # input shape is batch_size x seqeunce_length=64+1 x token size = 3\n",
        "      input = torch.concat((BOS, batch_of_sequences), dim=1) # <EOS> never input\n",
        "      out, _ = model(input.float())\n",
        "      # out = batch_size x sequence_size x 132 [0-129 pitch probs, 130 step, 131 duration]\n",
        "      # 129 as it is the pitch tokens from 0-127 + 2 for <BOS> and <EOS>\n",
        "      out_pitch = out[:, :, 0:130]\n",
        "      out_step = out[:, :, 130]\n",
        "      out_duration = out[:, :, 131]\n",
        "      targets = torch.concat((batch_of_sequences, EOS), dim=1) # <BOS> never output\n",
        "      targets = targets.float()\n",
        "      targets_pitch = targets[:, :, 0]\n",
        "      targets_step = targets[:, :, 1]\n",
        "      targets_duration = targets[:, :, 2]\n",
        "      # shape must be batch_size x # classes=130 x sequence_length\n",
        "      loss_pitch = criterion_pitch(out_pitch.reshape(batch_size, -1, out_pitch.shape[1]), targets_pitch.long())\n",
        "      loss_step = criterion_step(out_step, targets_step)\n",
        "      loss_duration = criterion_duration(out_duration, targets_duration)\n",
        "      total_loss = loss_pitch + loss_step + loss_duration \n",
        "      total_loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      # gather plotting data\n",
        "      num_iters += 1\n",
        "      losses.append(float(total_loss) / batch_size)\n",
        "      iters.append(num_iters)\n",
        "    # --- epoch ended ---\n",
        "    # # check point model\n",
        "    # if (checkpoint_path is not None) and num_iters > 0:\n",
        "    #   torch.save(model.state_dict(), checkpoint_path.format(num_iters))\n",
        "    # # track learning curve info\n",
        "    # iter_at_epoch.append(num_iters)\n",
        "    # train_acc.append(get_accuracy(model, train_data))\n",
        "    # val_acc.append(get_accuracy(model, valid_data))\n",
        "    # # report accuracies on train and validation set\n",
        "      print(\"Epoch %d. Iter %d. [Val Acc %.0f%%] [Train Acc %.0f%%, Loss %f]\" % (epoch, num_iters,0,0,float(total_loss.detach().numpy())))\n",
        "      # print(\"Epoch %d. Iter %d. [Val Acc %.0f%%] [Train Acc %.0f%%, Loss %f]\" % (epoch,\n",
        "      #     num_iters, val_acc[-1] * 100, train_acc[-1] * 100, float(total_loss.detach().numpy())))\n",
        "  return iters, losses, iter_at_epoch, train_acc, val_acc\n",
        "\n",
        "\n",
        "\n",
        "def plot_learning_curve(iters, losses, iter_at_epoch, train_accs, val_accs):\n",
        "    \"\"\"\n",
        "    Plot the learning curve.\n",
        "    \"\"\"\n",
        "    plt.title(\"Learning Curve: Loss per Iteration\")\n",
        "    plt.plot(iters, losses, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "    plt.title(\"Learning Curve: Accuracy per Iteration\")\n",
        "    plt.plot(iter_at_epoch, train_accs, label=\"Train\")\n",
        "    plt.plot(iter_at_epoch, val_accs, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "IEBnkQH-AqdS"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MusicGenRNN()\n",
        "train(model, train_set[:2], validation_set, num_epochs=120, batch_size=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4skpqviHJ_4",
        "outputId": "39b638aa-7311-4989-edf6-b17fa810dd74"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0. Iter 1. [Val Acc 0%] [Train Acc 0%, Loss 5.298226]\n",
            "Epoch 1. Iter 2. [Val Acc 0%] [Train Acc 0%, Loss 5.228052]\n",
            "Epoch 2. Iter 3. [Val Acc 0%] [Train Acc 0%, Loss 5.165895]\n",
            "Epoch 3. Iter 4. [Val Acc 0%] [Train Acc 0%, Loss 5.105360]\n",
            "Epoch 4. Iter 5. [Val Acc 0%] [Train Acc 0%, Loss 5.070100]\n",
            "Epoch 5. Iter 6. [Val Acc 0%] [Train Acc 0%, Loss 5.079233]\n",
            "Epoch 6. Iter 7. [Val Acc 0%] [Train Acc 0%, Loss 5.033733]\n",
            "Epoch 7. Iter 8. [Val Acc 0%] [Train Acc 0%, Loss 5.025095]\n",
            "Epoch 8. Iter 9. [Val Acc 0%] [Train Acc 0%, Loss 5.015596]\n",
            "Epoch 9. Iter 10. [Val Acc 0%] [Train Acc 0%, Loss 4.997922]\n",
            "Epoch 10. Iter 11. [Val Acc 0%] [Train Acc 0%, Loss 4.969443]\n",
            "Epoch 11. Iter 12. [Val Acc 0%] [Train Acc 0%, Loss 4.927563]\n",
            "Epoch 12. Iter 13. [Val Acc 0%] [Train Acc 0%, Loss 4.875422]\n",
            "Epoch 13. Iter 14. [Val Acc 0%] [Train Acc 0%, Loss 4.944679]\n",
            "Epoch 14. Iter 15. [Val Acc 0%] [Train Acc 0%, Loss 4.804642]\n",
            "Epoch 15. Iter 16. [Val Acc 0%] [Train Acc 0%, Loss 4.820263]\n",
            "Epoch 16. Iter 17. [Val Acc 0%] [Train Acc 0%, Loss 4.829783]\n",
            "Epoch 17. Iter 18. [Val Acc 0%] [Train Acc 0%, Loss 4.826009]\n",
            "Epoch 18. Iter 19. [Val Acc 0%] [Train Acc 0%, Loss 4.808628]\n",
            "Epoch 19. Iter 20. [Val Acc 0%] [Train Acc 0%, Loss 4.776479]\n",
            "Epoch 20. Iter 21. [Val Acc 0%] [Train Acc 0%, Loss 4.726247]\n",
            "Epoch 21. Iter 22. [Val Acc 0%] [Train Acc 0%, Loss 4.650990]\n",
            "Epoch 22. Iter 23. [Val Acc 0%] [Train Acc 0%, Loss 4.540061]\n",
            "Epoch 23. Iter 24. [Val Acc 0%] [Train Acc 0%, Loss 4.413857]\n",
            "Epoch 24. Iter 25. [Val Acc 0%] [Train Acc 0%, Loss 4.453946]\n",
            "Epoch 25. Iter 26. [Val Acc 0%] [Train Acc 0%, Loss 4.312013]\n",
            "Epoch 26. Iter 27. [Val Acc 0%] [Train Acc 0%, Loss 4.230771]\n",
            "Epoch 27. Iter 28. [Val Acc 0%] [Train Acc 0%, Loss 4.245182]\n",
            "Epoch 28. Iter 29. [Val Acc 0%] [Train Acc 0%, Loss 4.226789]\n",
            "Epoch 29. Iter 30. [Val Acc 0%] [Train Acc 0%, Loss 4.127403]\n",
            "Epoch 30. Iter 31. [Val Acc 0%] [Train Acc 0%, Loss 4.009631]\n",
            "Epoch 31. Iter 32. [Val Acc 0%] [Train Acc 0%, Loss 3.976509]\n",
            "Epoch 32. Iter 33. [Val Acc 0%] [Train Acc 0%, Loss 3.854780]\n",
            "Epoch 33. Iter 34. [Val Acc 0%] [Train Acc 0%, Loss 3.816327]\n",
            "Epoch 34. Iter 35. [Val Acc 0%] [Train Acc 0%, Loss 3.720975]\n",
            "Epoch 35. Iter 36. [Val Acc 0%] [Train Acc 0%, Loss 3.834573]\n",
            "Epoch 36. Iter 37. [Val Acc 0%] [Train Acc 0%, Loss 3.630728]\n",
            "Epoch 37. Iter 38. [Val Acc 0%] [Train Acc 0%, Loss 3.675484]\n",
            "Epoch 38. Iter 39. [Val Acc 0%] [Train Acc 0%, Loss 3.561941]\n",
            "Epoch 39. Iter 40. [Val Acc 0%] [Train Acc 0%, Loss 3.498014]\n",
            "Epoch 40. Iter 41. [Val Acc 0%] [Train Acc 0%, Loss 3.476338]\n",
            "Epoch 41. Iter 42. [Val Acc 0%] [Train Acc 0%, Loss 3.348607]\n",
            "Epoch 42. Iter 43. [Val Acc 0%] [Train Acc 0%, Loss 3.262408]\n",
            "Epoch 43. Iter 44. [Val Acc 0%] [Train Acc 0%, Loss 3.194299]\n",
            "Epoch 44. Iter 45. [Val Acc 0%] [Train Acc 0%, Loss 3.066986]\n",
            "Epoch 45. Iter 46. [Val Acc 0%] [Train Acc 0%, Loss 2.959782]\n",
            "Epoch 46. Iter 47. [Val Acc 0%] [Train Acc 0%, Loss 2.908692]\n",
            "Epoch 47. Iter 48. [Val Acc 0%] [Train Acc 0%, Loss 2.782404]\n",
            "Epoch 48. Iter 49. [Val Acc 0%] [Train Acc 0%, Loss 2.630200]\n",
            "Epoch 49. Iter 50. [Val Acc 0%] [Train Acc 0%, Loss 2.613769]\n",
            "Epoch 50. Iter 51. [Val Acc 0%] [Train Acc 0%, Loss 3.222926]\n",
            "Epoch 51. Iter 52. [Val Acc 0%] [Train Acc 0%, Loss 2.468983]\n",
            "Epoch 52. Iter 53. [Val Acc 0%] [Train Acc 0%, Loss 2.536538]\n",
            "Epoch 53. Iter 54. [Val Acc 0%] [Train Acc 0%, Loss 2.592849]\n",
            "Epoch 54. Iter 55. [Val Acc 0%] [Train Acc 0%, Loss 2.553499]\n",
            "Epoch 55. Iter 56. [Val Acc 0%] [Train Acc 0%, Loss 2.374327]\n",
            "Epoch 56. Iter 57. [Val Acc 0%] [Train Acc 0%, Loss 2.215230]\n",
            "Epoch 57. Iter 58. [Val Acc 0%] [Train Acc 0%, Loss 2.157513]\n",
            "Epoch 58. Iter 59. [Val Acc 0%] [Train Acc 0%, Loss 2.177058]\n",
            "Epoch 59. Iter 60. [Val Acc 0%] [Train Acc 0%, Loss 2.079988]\n",
            "Epoch 60. Iter 61. [Val Acc 0%] [Train Acc 0%, Loss 1.925167]\n",
            "Epoch 61. Iter 62. [Val Acc 0%] [Train Acc 0%, Loss 1.824778]\n",
            "Epoch 62. Iter 63. [Val Acc 0%] [Train Acc 0%, Loss 1.784141]\n",
            "Epoch 63. Iter 64. [Val Acc 0%] [Train Acc 0%, Loss 1.713359]\n",
            "Epoch 64. Iter 65. [Val Acc 0%] [Train Acc 0%, Loss 1.638317]\n",
            "Epoch 65. Iter 66. [Val Acc 0%] [Train Acc 0%, Loss 1.565251]\n",
            "Epoch 66. Iter 67. [Val Acc 0%] [Train Acc 0%, Loss 1.503525]\n",
            "Epoch 67. Iter 68. [Val Acc 0%] [Train Acc 0%, Loss 1.443155]\n",
            "Epoch 68. Iter 69. [Val Acc 0%] [Train Acc 0%, Loss 1.381211]\n",
            "Epoch 69. Iter 70. [Val Acc 0%] [Train Acc 0%, Loss 1.315719]\n",
            "Epoch 70. Iter 71. [Val Acc 0%] [Train Acc 0%, Loss 1.260603]\n",
            "Epoch 71. Iter 72. [Val Acc 0%] [Train Acc 0%, Loss 1.202279]\n",
            "Epoch 72. Iter 73. [Val Acc 0%] [Train Acc 0%, Loss 1.148935]\n",
            "Epoch 73. Iter 74. [Val Acc 0%] [Train Acc 0%, Loss 1.092464]\n",
            "Epoch 74. Iter 75. [Val Acc 0%] [Train Acc 0%, Loss 1.041868]\n",
            "Epoch 75. Iter 76. [Val Acc 0%] [Train Acc 0%, Loss 1.001382]\n",
            "Epoch 76. Iter 77. [Val Acc 0%] [Train Acc 0%, Loss 0.955745]\n",
            "Epoch 77. Iter 78. [Val Acc 0%] [Train Acc 0%, Loss 0.912314]\n",
            "Epoch 78. Iter 79. [Val Acc 0%] [Train Acc 0%, Loss 0.872757]\n",
            "Epoch 79. Iter 80. [Val Acc 0%] [Train Acc 0%, Loss 0.832842]\n",
            "Epoch 80. Iter 81. [Val Acc 0%] [Train Acc 0%, Loss 0.797443]\n",
            "Epoch 81. Iter 82. [Val Acc 0%] [Train Acc 0%, Loss 0.763809]\n",
            "Epoch 82. Iter 83. [Val Acc 0%] [Train Acc 0%, Loss 0.728206]\n",
            "Epoch 83. Iter 84. [Val Acc 0%] [Train Acc 0%, Loss 0.695247]\n",
            "Epoch 84. Iter 85. [Val Acc 0%] [Train Acc 0%, Loss 0.667998]\n",
            "Epoch 85. Iter 86. [Val Acc 0%] [Train Acc 0%, Loss 0.639968]\n",
            "Epoch 86. Iter 87. [Val Acc 0%] [Train Acc 0%, Loss 0.611021]\n",
            "Epoch 87. Iter 88. [Val Acc 0%] [Train Acc 0%, Loss 0.585276]\n",
            "Epoch 88. Iter 89. [Val Acc 0%] [Train Acc 0%, Loss 0.560430]\n",
            "Epoch 89. Iter 90. [Val Acc 0%] [Train Acc 0%, Loss 0.536355]\n",
            "Epoch 90. Iter 91. [Val Acc 0%] [Train Acc 0%, Loss 0.513362]\n",
            "Epoch 91. Iter 92. [Val Acc 0%] [Train Acc 0%, Loss 0.491589]\n",
            "Epoch 92. Iter 93. [Val Acc 0%] [Train Acc 0%, Loss 0.472503]\n",
            "Epoch 93. Iter 94. [Val Acc 0%] [Train Acc 0%, Loss 0.453996]\n",
            "Epoch 94. Iter 95. [Val Acc 0%] [Train Acc 0%, Loss 0.436048]\n",
            "Epoch 95. Iter 96. [Val Acc 0%] [Train Acc 0%, Loss 0.420472]\n",
            "Epoch 96. Iter 97. [Val Acc 0%] [Train Acc 0%, Loss 0.406069]\n",
            "Epoch 97. Iter 98. [Val Acc 0%] [Train Acc 0%, Loss 0.392314]\n",
            "Epoch 98. Iter 99. [Val Acc 0%] [Train Acc 0%, Loss 0.377290]\n",
            "Epoch 99. Iter 100. [Val Acc 0%] [Train Acc 0%, Loss 0.359932]\n",
            "Epoch 100. Iter 101. [Val Acc 0%] [Train Acc 0%, Loss 0.342299]\n",
            "Epoch 101. Iter 102. [Val Acc 0%] [Train Acc 0%, Loss 0.329184]\n",
            "Epoch 102. Iter 103. [Val Acc 0%] [Train Acc 0%, Loss 0.319298]\n",
            "Epoch 103. Iter 104. [Val Acc 0%] [Train Acc 0%, Loss 0.306954]\n",
            "Epoch 104. Iter 105. [Val Acc 0%] [Train Acc 0%, Loss 0.293420]\n",
            "Epoch 105. Iter 106. [Val Acc 0%] [Train Acc 0%, Loss 0.283518]\n",
            "Epoch 106. Iter 107. [Val Acc 0%] [Train Acc 0%, Loss 0.274282]\n",
            "Epoch 107. Iter 108. [Val Acc 0%] [Train Acc 0%, Loss 0.262960]\n",
            "Epoch 108. Iter 109. [Val Acc 0%] [Train Acc 0%, Loss 0.253565]\n",
            "Epoch 109. Iter 110. [Val Acc 0%] [Train Acc 0%, Loss 0.245400]\n",
            "Epoch 110. Iter 111. [Val Acc 0%] [Train Acc 0%, Loss 0.235599]\n",
            "Epoch 111. Iter 112. [Val Acc 0%] [Train Acc 0%, Loss 0.227367]\n",
            "Epoch 112. Iter 113. [Val Acc 0%] [Train Acc 0%, Loss 0.219834]\n",
            "Epoch 113. Iter 114. [Val Acc 0%] [Train Acc 0%, Loss 0.211246]\n",
            "Epoch 114. Iter 115. [Val Acc 0%] [Train Acc 0%, Loss 0.204028]\n",
            "Epoch 115. Iter 116. [Val Acc 0%] [Train Acc 0%, Loss 0.196721]\n",
            "Epoch 116. Iter 117. [Val Acc 0%] [Train Acc 0%, Loss 0.189167]\n",
            "Epoch 117. Iter 118. [Val Acc 0%] [Train Acc 0%, Loss 0.182608]\n",
            "Epoch 118. Iter 119. [Val Acc 0%] [Train Acc 0%, Loss 0.175539]\n",
            "Epoch 119. Iter 120. [Val Acc 0%] [Train Acc 0%, Loss 0.169027]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1,\n",
              "  2,\n",
              "  3,\n",
              "  4,\n",
              "  5,\n",
              "  6,\n",
              "  7,\n",
              "  8,\n",
              "  9,\n",
              "  10,\n",
              "  11,\n",
              "  12,\n",
              "  13,\n",
              "  14,\n",
              "  15,\n",
              "  16,\n",
              "  17,\n",
              "  18,\n",
              "  19,\n",
              "  20,\n",
              "  21,\n",
              "  22,\n",
              "  23,\n",
              "  24,\n",
              "  25,\n",
              "  26,\n",
              "  27,\n",
              "  28,\n",
              "  29,\n",
              "  30,\n",
              "  31,\n",
              "  32,\n",
              "  33,\n",
              "  34,\n",
              "  35,\n",
              "  36,\n",
              "  37,\n",
              "  38,\n",
              "  39,\n",
              "  40,\n",
              "  41,\n",
              "  42,\n",
              "  43,\n",
              "  44,\n",
              "  45,\n",
              "  46,\n",
              "  47,\n",
              "  48,\n",
              "  49,\n",
              "  50,\n",
              "  51,\n",
              "  52,\n",
              "  53,\n",
              "  54,\n",
              "  55,\n",
              "  56,\n",
              "  57,\n",
              "  58,\n",
              "  59,\n",
              "  60,\n",
              "  61,\n",
              "  62,\n",
              "  63,\n",
              "  64,\n",
              "  65,\n",
              "  66,\n",
              "  67,\n",
              "  68,\n",
              "  69,\n",
              "  70,\n",
              "  71,\n",
              "  72,\n",
              "  73,\n",
              "  74,\n",
              "  75,\n",
              "  76,\n",
              "  77,\n",
              "  78,\n",
              "  79,\n",
              "  80,\n",
              "  81,\n",
              "  82,\n",
              "  83,\n",
              "  84,\n",
              "  85,\n",
              "  86,\n",
              "  87,\n",
              "  88,\n",
              "  89,\n",
              "  90,\n",
              "  91,\n",
              "  92,\n",
              "  93,\n",
              "  94,\n",
              "  95,\n",
              "  96,\n",
              "  97,\n",
              "  98,\n",
              "  99,\n",
              "  100,\n",
              "  101,\n",
              "  102,\n",
              "  103,\n",
              "  104,\n",
              "  105,\n",
              "  106,\n",
              "  107,\n",
              "  108,\n",
              "  109,\n",
              "  110,\n",
              "  111,\n",
              "  112,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  117,\n",
              "  118,\n",
              "  119,\n",
              "  120],\n",
              " [2.649113178253174,\n",
              "  2.614025831222534,\n",
              "  2.5829477310180664,\n",
              "  2.552680015563965,\n",
              "  2.5350499153137207,\n",
              "  2.539616346359253,\n",
              "  2.516866445541382,\n",
              "  2.512547731399536,\n",
              "  2.507798194885254,\n",
              "  2.4989612102508545,\n",
              "  2.4847216606140137,\n",
              "  2.4637813568115234,\n",
              "  2.437711238861084,\n",
              "  2.472339630126953,\n",
              "  2.4023208618164062,\n",
              "  2.4101314544677734,\n",
              "  2.414891481399536,\n",
              "  2.4130046367645264,\n",
              "  2.4043140411376953,\n",
              "  2.388239622116089,\n",
              "  2.3631234169006348,\n",
              "  2.3254947662353516,\n",
              "  2.2700307369232178,\n",
              "  2.2069287300109863,\n",
              "  2.226973056793213,\n",
              "  2.156006336212158,\n",
              "  2.1153852939605713,\n",
              "  2.122591018676758,\n",
              "  2.1133947372436523,\n",
              "  2.0637013912200928,\n",
              "  2.0048155784606934,\n",
              "  1.988254427909851,\n",
              "  1.9273900985717773,\n",
              "  1.9081636667251587,\n",
              "  1.8604873418807983,\n",
              "  1.9172866344451904,\n",
              "  1.8153637647628784,\n",
              "  1.8377420902252197,\n",
              "  1.7809704542160034,\n",
              "  1.749006986618042,\n",
              "  1.7381689548492432,\n",
              "  1.6743035316467285,\n",
              "  1.6312041282653809,\n",
              "  1.5971492528915405,\n",
              "  1.5334930419921875,\n",
              "  1.4798911809921265,\n",
              "  1.4543458223342896,\n",
              "  1.3912022113800049,\n",
              "  1.3151001930236816,\n",
              "  1.3068842887878418,\n",
              "  1.611463189125061,\n",
              "  1.2344915866851807,\n",
              "  1.2682689428329468,\n",
              "  1.2964245080947876,\n",
              "  1.2767497301101685,\n",
              "  1.1871635913848877,\n",
              "  1.1076147556304932,\n",
              "  1.0787564516067505,\n",
              "  1.0885288715362549,\n",
              "  1.0399937629699707,\n",
              "  0.962583601474762,\n",
              "  0.9123892188072205,\n",
              "  0.8920705318450928,\n",
              "  0.8566792607307434,\n",
              "  0.8191583156585693,\n",
              "  0.7826255559921265,\n",
              "  0.7517625689506531,\n",
              "  0.7215774059295654,\n",
              "  0.6906053423881531,\n",
              "  0.6578593254089355,\n",
              "  0.6303013563156128,\n",
              "  0.6011394262313843,\n",
              "  0.5744673013687134,\n",
              "  0.5462321639060974,\n",
              "  0.520933985710144,\n",
              "  0.5006911754608154,\n",
              "  0.4778723418712616,\n",
              "  0.4561569392681122,\n",
              "  0.4363784193992615,\n",
              "  0.4164210557937622,\n",
              "  0.3987214267253876,\n",
              "  0.3819047212600708,\n",
              "  0.36410290002822876,\n",
              "  0.3476235270500183,\n",
              "  0.3339988887310028,\n",
              "  0.3199840784072876,\n",
              "  0.3055103123188019,\n",
              "  0.29263806343078613,\n",
              "  0.28021499514579773,\n",
              "  0.2681775391101837,\n",
              "  0.25668084621429443,\n",
              "  0.24579454958438873,\n",
              "  0.23625150322914124,\n",
              "  0.22699815034866333,\n",
              "  0.2180241346359253,\n",
              "  0.2102360874414444,\n",
              "  0.20303462445735931,\n",
              "  0.1961568295955658,\n",
              "  0.18864522874355316,\n",
              "  0.17996618151664734,\n",
              "  0.17114941775798798,\n",
              "  0.164591982960701,\n",
              "  0.1596490740776062,\n",
              "  0.15347710251808167,\n",
              "  0.14671024680137634,\n",
              "  0.14175912737846375,\n",
              "  0.13714110851287842,\n",
              "  0.1314799189567566,\n",
              "  0.1267826408147812,\n",
              "  0.12269982695579529,\n",
              "  0.11779961735010147,\n",
              "  0.11368340253829956,\n",
              "  0.10991718620061874,\n",
              "  0.10562275350093842,\n",
              "  0.10201399773359299,\n",
              "  0.09836052358150482,\n",
              "  0.0945834368467331,\n",
              "  0.09130389988422394,\n",
              "  0.08776962012052536,\n",
              "  0.08451355993747711],\n",
              " [],\n",
              " [],\n",
              " [])"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    }
  ]
}