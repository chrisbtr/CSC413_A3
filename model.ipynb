{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CYRKpOHv2ld"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicGenRNN(nn.Module):\n",
        "  def __init__(self, hidden_size=512, num_layers=1, bias=True):\n",
        "    super(MusicGenRNN, self).__init__()\n",
        "    # input # note, time and velocity \n",
        "    # size 128 note, 1, 128 velocity -> flatten to size of 128+1+128=257\n",
        "    self.one_hot_size = 128\n",
        "    self.note_emb_size = 128\n",
        "    self.velocity_emb_size = 128\n",
        "    self.input_size = self.note_emb_size + self.velocity_emb_size + 1\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = self.one_hot_size * 2 + 1\n",
        "\n",
        "    # identiy matrix for generating one-hot vectors\n",
        "    self.ident = torch.eye(one_hot_size) # recurrent neural network\n",
        "    self.note_embedding = nn.Linear(self.one_hot_size, self.note_emb_size, bias=False)\n",
        "    self.velocity_embedding = nn.Linear(self.one_hot_size, self.velocity_emb_size, bias=False)\n",
        "\n",
        "    #self.rnn = nn.LSTM(input_size, hidden_size, num_layers,bias, batch_first=True, dropout) # a fully-connect layer that outputs a distribution over\n",
        "                    # the next token, given the RNN output\n",
        "    self.rnn = nn.LSTM(self.input_size, hidden_size, num_layers, bias, batch_first=True, dropout)\n",
        "    self.decoder = nn.Linear(hidden_size, self.output_size)\n",
        "\n",
        "  def forward(self, input, hidden_in=None):\n",
        "    inp_note, inp_time, inp_velocity = input \n",
        "    one_hot_note = self.ident[inp_note] # generate one-hot vectors of input\n",
        "    one_hot_velocity = self.ident[inp_time]\n",
        "    embedded_note = self.note_embedding(one_hot_note)\n",
        "    embedded_velocity = self.velocity_embedding(one_hot_velocity)\n",
        "    inp = torch.concat((embedded_note, inp_time, embedded_velocity))\n",
        "    output, hidden_out = self.rnn(inp, hidden_in) # get the next output and hidden state\n",
        "    output = self.decoder(output) # predict distribution over next tokens\n",
        "    return output, hidden_out"
      ],
      "metadata": {
        "id": "gaX60joVwRaT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}