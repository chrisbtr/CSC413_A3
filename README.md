# CSC413 Final Project
## Group Name: Boots and Cats
## Members: 
- **Jackson Joseph Hoogenboom**
- **Chris Botros**
- **Brad Hebert**

# Introduction
Brad

# How to run our model
1. Download the data from https://www.kaggle.com/datasets/soumikrakshit/classical-music-midi, run it through our `data_parser.py` functions by doing the below. Make sure to store each of the training, test, and validation sets in a file called `data.pickle` in the root of this directory.
    1. Sub steps here***
    2. more steps
2. Once `data.pickle` has been made and stored in the root of this directory, update file paths tagged as `# UPDATE` in the `model_and_training.ipynb` file by following the comments near them in the code. Once this is done, to train the model run please run all the cells up to and including the cell with the `train()` function and the cell towards the bottom of the file with the `sample_sequence()` function definition in it. This cell is the first one under the **Sampling/Producing music** section.
3. Now that everything needed has been run, you may run the cell that trains the model and calls the `train()` function. This cell is below the cell that defines the `train()` function as well as the other functions used by `train()`. If you want to switch up the parameters to train with feel free to look at the doc-string of the `train()` function. The ones currently in this cell are the ones we used to define and train the final model, so the output saved here was our final models output. Note, that though these training statements have accuracy scores as low percents this is a good thing based on how we defined accuracy, see the results section below for more. Alternatively, you can use the provided weights in `report-weights.pk` to preload the model with, you can find the cell to do this below the cell where `train()` is called, in the **Training** section. 
4. Now that the model is trained up or loaded with weights you can head to the **Sampling/Producing music** section and begin creating songs. There is a cell that shows an example call of how to do this below the cell where `sample_sequence()` is defined, please read the doc-string for how to use this function, if you are unsure.
5. Make music doing this for as long as you want, enjoy! At the bottom of this file you will find our functions for computing results under the aptly named **Result Computations** section, you may use these if you would like to compute your models results if you happened to have trained your own model.

*NOTE One: If the instruction order on how to run the cells is confusing they have text cells above them explaining the order to run them in as guidance*

*NOTE Two: the `device` variable is used throughout the code, please ensure this variable has been defined before calling any functions as otherwise they may error.*

# Model
&nbsp; &nbsp; In this section we will analyze how our model functions for its two use cases and explain how it works to generate music sequences in each case. We analyze only the final version of the model we settled on in this report.

## Model Diagram
&nbsp; &nbsp; In this section we will outline the structure of our model for its two use cases, the first use case is generating classical piano music from scratch, the second use case is where the model is first fed a sample sequence of classical midi piano music as a Tensor, in the shape lx3, where `l` is the sequence length and 3 is the dimension of each note in the shape of `[pitch index, step, duration]` where pitch is a integer between 0-127 inclusive and step and duration are continous values for the note. To make a sequence into this shape please see the data section of this file. Using this as inspiration (and to update the models hidden state) the model then generates music, using its built up knowledge of this input as some guidance. Below in the first diagram (Figure 1) is the unrolled diagram of our Recurrent Neural Network Model specifically in the case where it generates music from scratch. In Figure 2 we have our unrolled model for the case of using an input sequence as guidance. To make a model of this shape we create an instance of our models class by calling like so `MusicGenRNN(num_rnn_layers=3)`.  

**Figure 1:** Our Unrolled Generative RNN Model for generating music from scratch.
![Our RNN Model](./ReadMe-Media/model-diagram.png "Our Unrolled Generative RNN model generating from scratch")

**Figure 2:** Our Unrolled Generative RNN Model for generating music using an input sequence as guidance.
![Our RNN Model](./ReadMe-Media/model-diagram-fed-input.png "Our Unrolled Generative RNN model generating from scratch")

&nbsp; &nbsp; From Figure 1 and 2, it is seen that our model is made up two key components the encoder layers, and the decoder layers. The encoder component is made up of the following layers: 3 LSTM RNN layers, along with the pitch embedding matrix, and pitch index to one hot vector matrix. On the other side we have the decoder component which is made up of the two MLPs for generating the step and duration of the note respectively, from the last encoder layers output. Also in the decoder component is the two pitch decoder layers with the ReLU activation function between the two used for predicting/generating the distribution of pitches to sample from for the next note, which is later fed in as input.

&nbsp; &nbsp; We will now outline how our model works to generate the musical sequences, this has two possible use cases, we will start with the use case of feeding the model an input sequence as it encompasses the use case of generating from scratch, and we will mention in our discussion when these two differ, and how.

&nbsp; &nbsp; To begin the case where we feed the model an input sequence we do this one token at a time, as seen in Figure 2, we first feed it token/note 1 from input sequence which is a token that consists of the pitch number, which the key we want it to play, as well as the notes step and duration values. To get an input sequence of this style to feed in please see the data section in this readme which will describe the data_parser.py file and its functions. We will also start off by feeding its LSTM hidden states a value of None since this is the first token/note it sees as input. Once this input has propagated through all 3 LSTM layers and the hidden states are updated/computed for this time step (the first time step) we ignore the last LSTM layers output and feed these hidden state values in to the next hidden states as the previous hidden states values for the model along with the next input token/note which is composed of the next values for pitch, step, and duration. We do this to compute the hidden state values for the second time step. We continue this process of feeding in the next token/not from the input sequence and previous hidden states to the model while ignoring the output until we run out of tokens in the input sequence. 

&nbsp; &nbsp; It is at this point where the from scratch generation would start and where we actually start generating output in both use case, these two methods are the same from here on with the following small difference. For the "from scratch use case" we start this generation off by feeding in the hidden states as None valued input like we did at the beginning of the "feeding in input sequence use case". In the "feeding in input sequence use case" however we instead supply the hidden states computed at the end of feeding in the input sequence, as this will contain our encoded memory of the input sequence for which we want to build off of. Moving forward from this, both use cases becomes the same. In both cases once the previous hidden states are fed in, we feed in the begin sequence token and this gets embedded and fed in to the encoding layers so they produce an output to represent the initial note/token distribution. The encoding components output is then fed into the decoder layers. Each decoder layer generates a different portion of the output token/note. There is an MLP decoder layer that generates the step and duration of the note respectively. The other two MLP decoder layers along with a ReLU activation between them generates a categorical distribution for the pitch of the note to be played, this is then sampled from to actually pick the note played. The three components are then combined to produce the output note, this output token/note is then fed back into the model as the next input and we feed in the hidden states we just produced as well to then generated the next token/note in the output sequence. We repeat this for the desired length specified by the user and use the output tokens/notes generated in order to assemble a sequence which we then compose into a midi file using our data parsing tools. This is how our model works.

&nbsp; &nbsp; To perform the above once the model is trained, or loaded with weights head to the **Sampling/Producing music** and look at the code cells there and the docstring to figure out how to interact with the model and make original classical piano music midi files.

## Model Parameter Analysis

&nbsp; &nbsp; Here we analyze the parameters that make up the model. To do this we will go layer by layer. First we have matrix which converts our pitch index into a one hot vector, this is not a trainable parameter but it is part of the model non-the-less. This parameter is an identity matrix of size 129x129 as we have a 128 possible pitches plus our begin sequence token, and each pitch index must be convertible to a one hot vector hence the dimensions chosen. Getting into actually learnable parameters we have our pitch embedding layer (a Linear MLP layer with no bias) it is effectively a matrix that takes the 129x1 one hot vectors in as input and extracts the embedded representation of the pitch which is a continuous vector of size 130x1. Thus this layer gives use 130x129 learnable parameters as it is a fully connected layer. We then concatenate this embedded vector with the continuous step and duration values from the input note/token, giving us a 132x1 input vector into the first LSTM RNN encoding layer.

&nbsp; &nbsp; This layer (the first LSTM RNN encoding layer) gives us 2048x132, 2048x512 and 2048 * 2 parameters. These parameters come from firstly the matrix used to compute the portion of the i_t, f_t, o_t, and g_t vectors used in the LSTM computation as defined in the pytorch LSTM docs which represent the various pieces of the LSTM computation. The input vector of shape 132x1 is used along with other pieces to compute each of these vectors, as such this is where we get the 2048x132 parameter matrix from, as it is multiplied with the input vector as part of the computation of these vectors. The 2048x512 parameter matrix comes from the portion of the computation of the i_t, f_t, o_t, and g_t vectors which uses 512x1 hidden vector as the hidden vector for this layer is multiplied by the 2048x512 parameter matrix. Lastly, the two 2048 parameter vectors come from the bias added to each of these matrix multiplications. We also note that we have 2048xX here as each of the i_t, f_t, o_t, and g_t vectors is 512x1 (since we have a hidden state size of 512 for the LSTM layers) and we compute them by concatenating them on top of each other which gives a 4*512x1 = 2048x1 vector. Which then has the corresponding non-linearity's applied to the corresponding portion of the 2048x1 vector for each vector, this is then used to c_t and h_t a 512x1 vector fed into the next hidden state and used in the next time steps computation. The other two embedding layers give use two 2048x512 parameter matrices and two 2048x1 parameter vectors each. This for similar reasons as the first embedding layer expect we have two 2048x512 parameter matrices instead as the input is now the same size as the hidden since the first encoding LSTM layer outputs a 512x1 vector which is the same size as its hidden state.

&nbsp; &nbsp; Moving on to the Decoder layers these give us the following trainable parameters. Firstly we get two 512x1 parameter matrices and 1x1 parameter vectors from the two fully connected layers that take the final embedding layers 512x1 output vector and compute the step and duration respectively. This gives us a 512x1 parameter matrix and a 1x1 parameter vector for each of these layers as we produce one output each from a 512x1 vector input, and we have a bias. We also get a 512x300 parameter matrix from the first pitch fully connected decoding layer as it takes in 512x1 vector from the last encoding layer and outputs a 300x1 vector to an ReLU activation function which is then passed into the other pitch fully connected layer. We also have a bias in that layer which gives us 300x1 parameter vector. Lastly, the last pitch decoder layer gives use a 129x300 and 129x1 parameter matrix and vector, as it is a fully connected layer with a bias that takes the 300x1 output vector from the previous pitch decoder layer and computes the distribution of the 129 possible pitch values. All of these learnable parameters sum together to give us 130x129 + 2048x132+2048x512+2048+2048 + 2048x512+2048x512+2048+2048 + 2048x512+2048x512+2048+2048 + 300x512+300 + 129x130+129 + 1x512+1 + 1x512+1 = 5714099 many learnable parameters total.

## Model Output Examples

&nbsp; &nbsp; In this section we will analyze a few of the generated classical piano music midi sequences saved as midi files produced by our model. Since we generated music files we unfortunately can not embed them in this readme, so to check them out some examples can be found under the `./Songs` directory. This directory has a `Good-Examples` subdirectory and a `Bad-Examples` subdirectory. Below you can find tables describing the contents of these directories.

**Table 1: Good Examples**
| File Name/Song Title | Description | Notes |
|----------------------|-------------|-------|
| temp8-no-in-med.mid  | A song generated with no given input sequence i.e. made from scratch with temperature about 0.8 and length medium | This was selected as good due to the pleasant sound note choices as well as the varity of durations of notes played making it a good example.
|temp8-test-49-med.mid  | A song generated with input as example 49 from the test set with temperature 0.8 and medium length | This song opens with the test example fed in an then the once it finishes everything the model generated given this input is appended and plays after it, this transition occurs at around 17 seconds. This song was selected as after listening to it as the output sounded pretty good as it did not play the same key repeatedly and made some consistent good note choices. See our Results section for more information on qualities as to why we classed this a good example. 
| temp10-test-98-med.mid | A song generated with input as example 98 from the test set with temperature 1.0 and medium length | This song opens with the test example fed in an then the once it finishes everything the model generated given this input is appended and plays after it, this transition occurs at around 8 seconds. This song was selected as after listening to it the model generated a variety of good sounding notes. Unlike the above the test input pattern though the pattern was not matched as closely, but it was still pretty good. See our Results section for more information on qualities as to why we classed this a good example. 

For more Good examples please see the `"Other good examples"` subdirectory under the `Good-Examples` subdirectory.

**Table 2: Bad Examples**
| File Name/Song Title | Description | Notes |
|----------------------|-------------|-------|
|temp4-no-in-med.mid | A song generated with no input sequence given with temperature 0.4 and medium length/number of tokens required | This song was classed as bad as it plays the same note repeatedly with little variation, we found this to be an issue often when training models and it is especially an issue with most the models we had generate input with temperature < 0.8
|  temp8-no-in-long.mid | A song generated with no input as with temperature 0.8 and long length/number of tokens required | This song opens a seemingly normal sequence but quickly transitions to rapidly playing high pitch notes which we found to be an issue when asking the model for longer sequences/ones with many notes. See our Results section for more information on qualities as to why we classed this a bad example. 
| temp8-test-91-med.mid | A song generated with input as example 91 from the test set with temperature 0.8 and medium length | This song opens with the test example fed in an then the once it finishes everything the model generated given this input is appended and plays after it, this transition occurs at around 7 seconds. This song was selected as bad after listening to it at around 10 seconds it transitions to rapidly playing many high pitch notes all at once very quickly which does not sound vary appealing. See our Results section for more information on qualities as to why we classed this a bad example. 


# Data
Chris 
- be sure to explain why we used certain data split
- mention the graphs are loss for each batch, not an average.

# Training
chris

# Results
&nbsp; &nbsp; In this section we will explain how we quantified our results, what results we obtained, and why we obtained the results we got. To begin we will first explain how we measured our results both quantitatively and qualitatively. To measure our results quantitavitvely we found this challenging as we were our model was generative meaning it should produce output it had never, and none of us were really fimilar with how to quantitaively measure this. We ended up deciding on two types of measures.

## How we Measured Results
&nbsp; &nbsp; Firstly the "loss", the loss is computed as the sum of three different loss components computed on each part of the output token. We calcualte a loss on the pitch using logistic cross entory since this is a categorical prediction the model is making and we calculate a MSE loss on both the step and duration components respectivley. We compute the loss in a similar style to teacher forcing where the we use the next actual token compared to the one generated for that time step by the model, moreover we don't let the loss accumulate from mispredictions since at each time step we enter in the correct token as input no matter what, just like in teacher forcing. To get the full loss value we averaged the loss over all the samples in each set respectively these results can be seen below.

&nbsp; &nbsp; The second measure we choose to gauge results was an accuacy measure comparing the genearted distrubtion of notes with the distributions in the given data sets, we used the follwong source "Techniques to measure probabilty distribution similarity" by Renu Khandelwal to build a understand of how to compute the similarity of distributions [1]. Our idea here was to compare the distribution of generated notes pitch, the distribution of generated notes step, and the distribution of generated notes duration with the distubtion of each these compontents of the notees in the both the training and validation set during training, and the testing set once we chose a model to go with. We chose to do this as we figured the more accurate our model would the be the better/closer its produced distribtioon of notes would follow the distribtuion in the data set. The measure we choose to compare these distribtuins is the Jensen Shannon Divergence(JSD), we choose to go with this measure of similarity as it is bounded between 0 and 1. The closer the JSD value is to 0 the more similar the distribtuions are and the the closer to 1 the more divergent they are. To compute these values we genertated histograms as used these as the probability distribtuions. We made histograms for each of the components, i.e. the pitch, step and duration deescribed above and built them by using 50 smaples generated from the model of length X and used JSD to compare these against the histograms made on each set of the data. We only went with 50 samples as the distribtuions are normalized thus we dont need to sample/generate the same amount of data as what is in each set to get an accurate distribtuion. JSD is built on top of Kullback–Leibler Divergence or KL divergence. JSD is thee symetric version of KL divergence and is used to measure similarity of the distribtuions by quantifing the information lost when using one distribtuion over anther. to see how it is computed pleease see the functions `_kullback_leiber_divergence()` and `janseen_divergence()` in `model_and_training.ipynb` under the training section, they are in this section as we used them to gaugee accuaracy during training. The rest of our code to compute results can be found under the results section.

&nbsp; &nbsp; Since we were made a generative music model we also qualittavivleey gauged the qualitity of our generated music. As deescribed in our introducation our task was generating a piano track from a midi file trained on fmaous classical composers midi files. So to gauge our quality we listened for various good and bad patterns good patterns includeed, pattern repition in notes, as most songs have some kind of piano riff that would be repeated a few times. We also listened for bad patterns suchs as wrong notes or keys getting played, for example playing high notes and then playing one really low note. Other negaitive patterns listened for was wether it soundeed like someone was just hitting as many keys as possible as fast as possible with no structure, and also just playing the smae key over and over again. None of us were music effeciandos so we did not have alot of experience with judging musics so we just used these simple points to look for grading our generation. 

[1] "Techniques to measure probabilty distribution similarity" by Renu Khandelwal https://medium.com/geekculture/techniques-to-measure-probability-distribution-similarity-9145678d68a6

## Our Result Values

## Results Discussion

After having trained many many models and listened to their outputs, we believed the one we settled on perfromed reasonably and as good as we could achieve given the difficulty of the problem. wee expereinced in training many models that did X, y , or Z. this model we settled with we found best due to these reasons .... . Some issues we still found with this model are ... . Going forward we could try and do x, y, z, transformer. 

# Ethical Considerations
Brad
- metnion overfit models exact clonning 


# Authors
-  Jackson J. Hoogenboom (hoogenb2) 
-  Bradley D. Hebert (hebertbr)
-  Chris Botros (botrosc2)